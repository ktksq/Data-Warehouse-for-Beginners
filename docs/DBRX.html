<!doctype html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>データブリックスが作った新しいLLM「DBRX」って、一体どんなモデル？ | はじめてのデータウェアハウス</title>
    <link rel="stylesheet" type="text/css" href="css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="css/webstyle.css" />
    <link rel="next" title="著者紹介" href="contributors.html">
    <link rel="prev" title="既存データウェアハウスからDatabricksへの実践的な移行方法と成功のためのヒント" href="dbsql-migration.html">
    <meta name="generator" content="Re:VIEW Starter">
  </head>
  <body>
    <div class="page-outer">
      <nav class="side-content">
                <a class="nav-title" href="index.html">はじめてのデータウェアハウス</a>
<ul class="toc toc-1">
    <li class="toc-chapter"><a href="./preface.html">まえがき</a></li>
    <li class="toc-chapter"><a href="./dbsql-general.html">1 Databricks SQL の概要</a></li>
    <li class="toc-chapter"><a href="./unitycatalog.html">2 Unity Catalogではじめるデータマネジメント</a></li>
    <li class="toc-chapter"><a href="./data-engineering.html">3 データエンジニアリング</a></li>
    <li class="toc-chapter"><a href="./lakeview.html">4 Lakeviewによるデータ可視化とBIダッシュボード</a></li>
    <li class="toc-chapter"><a href="./realtime-app-dbsql.html">5 Databricks SQLとRaspberry Piを使ってIoTデータダッシュボードを作る</a></li>
    <li class="toc-chapter"><a href="./SQLextention.html">6 Databricks SQLの拡張機能</a></li>
    <li class="toc-chapter"><a href="./dbsql-perf-tuning.html">7 Databricks SQL パフォーマンス・チューニング Tips</a></li>
    <li class="toc-chapter"><a href="./dbsql-serverless-nw-security.html">8 Databricks SQLサーバーレスのネットワークセキュリティ</a></li>
    <li class="toc-chapter"><a href="./dbsql-migration.html">9 既存データウェアハウスからDatabricksへの実践的な移行方法と成功のためのヒント</a></li>
    <li class="toc-chapter"><a href="./DBRX.html">データブリックスが作った新しいLLM「DBRX」って、一体どんなモデル？</a></li>
    <li class="toc-chapter"><a href="./contributors.html">著者紹介</a></li>
</ul>
      </nav>
      <div class="page-inner">
        <header class="page-header">
        </header>
        <main class="page-main">
<h1 class="boldlines center twolines"><a id="hA"></a><span class="secno">付録A</span> <br/>データブリックスが作った新しいLLM「DBRX」って、一体どんなモデル？</h1>
<div class="column">

<h4><a id="column-1"></a>データブリックスが作った新しいLLM「DBRX」って、一体どんなモデル？</h4>
<p>ChatGPT が2022年11月にリリースされて以来、生成AI技術は世界中に急速に普及し、多くの企業がこの技術をビジネスに活用できないかと検討を始めました。電子情報技術産業協会（JEITA）のレポート<sup><a id="fnb-JEITA" href="#fn-JEITA" class="noteref" epub:type="noteref">*1</a></sup>によると、生成AI市場は年平均53.3%のペースで成長しており、2030年には約2,110億ドルに達する見込みとのことです。これは2023年の市場規模の約20倍にあたり、非常に大きなポテンシャルを秘めた技術であることが改めて明らかになりました。</p>
<p>そんな生成AI市場における2023年から2024年にかけての大きな変化の一つは、オープンソース（OSS）技術への注目の高まりです。Andreessen Horowitz（a16z）のレポート<sup><a id="fnb-A16Z" href="#fn-A16Z" class="noteref" epub:type="noteref">*2</a></sup>では、2023年には80%以上の企業が ChatGPT のようなクローズドソースの生成AIを使用していたのに対し、2024年には調査回答者の約46%がOSSモデルを好む、または強く好むと述べています。また、AIリーダーの約60%が、ファインチューニングされたOSSモデルがクローズドソースモデルの性能に匹敵する場合、OSSの利用を増やすか、乗り換えることに興味があると述べています。さらに、同レポートでは、OSSが注目される理由として「コントロール（60%）」と「カスタム性（30%）」が大部分を占めている、と記されています。コントロールとは、社内データのセキュリティや生成AIの出力を自らの手で管理したいというニーズを指し、カスタム性とは、ビジネスに合わせてモデルを調整したいという要望になります。意外なことに、「コスト」はこれらの二つに比べると重視されてないように見えますが、これは将来、生成AIが生み出す価値がそのコストを大きく上回るだろうというリーダーたちの考えを反映しているのかもしれません。</p>
<p>いずれにしても、このような市場の需要と期待に応える形で、OSSモデルの開発競争も激化しています。2023年以降、Meta社による LLaMA のリリースを皮切りに、主要なものに限定をしても、新たなOSSの大規模言語モデル（LLM）が毎月のように登場しています（<span class="tableref"><a href="./DBRX.html#llm_history">表A.1</a></span>）。</p>
<div id="llm_history" class="table">
<p class="caption">表A.1: 2023年以降にリリースされた代表的なOSS LLM (Wikipediaより引用)</p>
<table>
<tr class="hline"><th>リリース年月</th><th>モデル名</th><th>開発者</th></tr>
<tr class="hline"><td>2023/02</td><td>LLaMA</td><td>Meta</td></tr>
<tr class="hline"><td>2023/03</td><td>Cerebras-GPT</td><td>Cerebras</td></tr>
<tr class="hline"><td>2023/03</td><td>Falcon</td><td>Technology Innovation Institute</td></tr>
<tr class="hline"><td>2023/03</td><td>OpenAssistant</td><td>LAION</td></tr>
<tr class="hline"><td>2023/07</td><td>Llama 2</td><td>Meta</td></tr>
<tr class="hline"><td>2023/09</td><td>Falcon 180B</td><td>Technology Innovation Institute</td></tr>
<tr class="hline"><td>2023/09</td><td>Mistral 7B</td><td>Mistral AI</td></tr>
<tr class="hline"><td>2023/11</td><td>Grok-1</td><td>x.AI</td></tr>
<tr class="hline"><td>2023/12</td><td>Mixtral 8x7B</td><td>Mistral AI</td></tr>
<tr class="hline"><td>2023/12</td><td>Phi-2</td><td>Microsoft</td></tr>
<tr class="hline"><td>2024/01</td><td>Eagle 7B</td><td>RWKV</td></tr>
<tr class="hline"><td>2024/02</td><td>Gemma</td><td>Google DeepMind</td></tr>
<tr class="hline"><td>2024/03</td><td>DBRX</td><td>Databricks</td></tr>
<tr class="hline"><td>2024/04</td><td>Llama 3</td><td>Meta</td></tr>
</table>
</div>
<p>さて、少々前置きが長くなりましたが、データブリックスもこのダイナミズムに応じて、2024年3月27日に<strong>「DBRX」</strong>という新しい OSS LLM を発表しました。このモデルはデータブリックスのR&amp;D部門、Mosaicリサーチチームによってゼロから開発され、132B（1320億個）のパラメーターを有しています。また、商用利用が可能で、日本語にも対応しています。本コラム執筆時点（2024年5月8日現在）では、DBRX Base（事前学習済みモデル）とDBRX Instruct（インストラクションチューニング済み）の2種類のモデルがリリースされており、どちらも HuggingFace や Databricks Marketplace からダウンロードして使用することが可能です。モデルのソースコードも GitHub に公開されてますので、具体的な実装内容を確認できます。</p>
<p>そんな DBRX の特徴はというと、ズバリ<strong>「高精度、かつ、高速」</strong>であることです。まず、&quot;高精度&quot;に関してですが、モデルの発表時点で、MMLU（言語理解）、HumanEval（プログラミング）、GSM8K（数学）などの主要ベンチマークで他のすべての OSS LLM を上回るスコアを達成していたことから（<span class="imgref"><a href="./DBRX.html#dbrx_benchmark">図A.1</a></span>）、SOTA（State-of-the-Art）なモデルとして公開されました<sup><a id="fnb-DBRX_SOTA_NOTE" href="#fn-DBRX_SOTA_NOTE" class="noteref" epub:type="noteref">*3</a></sup>。また、OpenAI GPT-3.5 や Google Gemini 1.0 Pro などのクローズドモデルと比べても同等以上の性能を示しており（<span class="imgref"><a href="./DBRX.html#dbrx_benchmark">図A.1</a></span>）、OSSがクローズドモデルと対等な勝負ができる新たな時代の幕開けとも考えることができます。ベンチマークなどの詳細については、ぜひ公式ブログ<sup><a id="fnb-DBRX_BLOG" href="#fn-DBRX_BLOG" class="noteref" epub:type="noteref">*4</a></sup>もご参照ください。</p>
<div id="dbrx_benchmark" class="image">
<img src="images/DBRX/dbrx_benchmark.png" alt="DBRXのベンチマーク結果（公式ブログより引用）" class="img" />
<p class="caption">
図A.1: DBRXのベンチマーク結果（公式ブログより引用）
</p>
</div>
<p>続いて、”高速”という特徴についてですが、まずこれは推論や学習処理の速さを意味します。そして、なぜ速いのかというと、DBRX が採用している<strong>「MoE（Mixture of Experts）」</strong>というアーキテクチャにカラクリがあります。「えむ・おー・いー？、、、初耳だ。」という方向けにまずは大枠を説明しますと、そもそも MoE とは1990年代に開発された技術でして、2021年ごろからAIの言語モデルへ適用がされ始めました。なぜ急に MoE が使われ始めたのか、その理由は後述しますが（←実はこれが本コラムのメイントピックだったりします）、いずれにしても近年では、Mixtral、Grok-1、Gemini 1.5 Pro などの主要な LLM へ採用されており、その有用性が認められています。</p>
<p>それでは、なぜ MoE がモデルの処理速度に寄与するのでしょうか？その解説に入る前に、LLM が持つ課題やそれを取り巻く背景を少し振り返りましょう。現在の LLM の多くは、2017年に登場した Transformer のデコーダー部分を基にしています。モデルの内部構造は、主に Transformer ブロックという層を直列に複数並べて構成されています（<span class="imgref"><a href="./DBRX.html#mixture-of-experts-moe-for-machine-learning_0">図A.2</a></span> Subfigure A）。ブロックの具体的な数はモデルの設計に依存しますが、例えば、Llama2-70b では80層、DBRX では40層が使用されています。Transformer ブロックの内部は、Attention 機構と Feed Forward Network（FFN）が順番に実行される形で構成されています（<span class="imgref"><a href="./DBRX.html#mixture-of-experts-moe-for-machine-learning_0">図A.2</a></span> Subfigure B）。そして、ここからが重要なのですが、従来の LLM では、FFN が、比較的大きなサイズの単一の GLU（Gated Linear Unit）で構成されています（<span class="imgref"><a href="./DBRX.html#mixture-of-experts-moe-for-machine-learning_0">図A.2</a></span> Subfigure C）。ここでは、このようなFFNを有した従来のLLMを「密なモデル（Dense Model）」と呼びます。元来、モデルの精度を高めるためのアプローチとして、この「密なモデル」をスケールアップ（＝パラメーター数を増やす）する試みが行われていました。これは、大きなモデルほどより多くの情報を捉え、複雑なパターンを学習する能力がある、という有名な「スケーリング則（Scaling Law）」に則った考え方ですが、当然、計算コストとのトレードオフが発生します。つまり、モデルのパラメーター数が増加すると、モデルがデータを処理する際に必要な計算量が増え、全体の演算負荷が高まります。また、データを処理するのに必要なメモリと時間も増加し、処理効率が低下します。よって、<strong>密なモデルのままでは、モデルの高精度と高速処理を両立するのが困難</strong>、という課題がありました。</p>
<div id="mixture-of-experts-moe-for-machine-learning_0" class="image">
<img src="images/DBRX/mixture-of-experts-moe-for-machine-learning_0.png" alt="Mixture of Experts（MoE）（公式ブログより引用）" class="img" />
<p class="caption">
図A.2: Mixture of Experts（MoE）（公式ブログより引用）
</p>
</div>
<p>このような背景の中で、MoE に注目が集まり始めました。MoE の発想は、「全知全能の巨大な神様一人に問題を解かせるのではなく、特定の知識に長けた複数の小さな専門家を寄せ集めて問題を解かせよう」というものです。具体的には、FFN に関して、従来では単一の GLU に全ての情報を溜め込み、かつ、全てのパラメーターを総動員して演算を行なっていましたが、小さな GLU を複数個、並列に演算させるという形に変更します。<span class="imgref"><a href="./DBRX.html#mixture-of-experts-moe-for-machine-learning_0">図A.2</a></span> Subfigure D をご覧ください。比較的小さなサイズの GLU がN個、並列配置され、Router と呼ばれるコンポーネントが入力データに基づいて適切な GLU を動的に複数個（N未満の数）選択しています。そして、この仕組みの中では GLU はエキスパートと呼ばれることから、「Mixture of Experts」と名付けられているわけです。いずれにしても、このシステムにおいては、一部のエキスパートのみが常時稼働すれば良いため、使用されるパラメーター数が大幅に削減され、処理効率が向上します。DBRX の場合、<strong>全16個のエキスパートの中から毎回4個だけが選ばれるので、パラメーター数で言うと、全132B のうち 36B のみが実際に使用されます。</strong>学習データの情報は 132B の非常に多数のパラメーターに保持しておけますし、それらを使用する際は 36B だけ動かせばいい、ということで、Llama2-70b のような 70B のモデルよりも高い精度が実現できる<sup><a id="fnb-MOE_VS_DENSE" href="#fn-MOE_VS_DENSE" class="noteref" epub:type="noteref">*5</a></sup>うえ、演算量が約半分となり、推論速度にして約2倍高速であることが確認されています<sup><a id="fnb-DBRX_BLOG" href="#fn-DBRX_BLOG" class="noteref" epub:type="noteref">*4</a></sup>。さらに、DBRX は TensorRT-LLM や vLLM などの推論エンジンにも対応しており<sup><a id="fnb-DBRX_INFERENCE_OPT" href="#fn-DBRX_INFERENCE_OPT" class="noteref" epub:type="noteref">*6</a></sup>、コミュニティ開発による量子化版 DBRX も登場していますので<sup><a id="fnb-DBRX_QUANTIZATION" href="#fn-DBRX_QUANTIZATION" class="noteref" epub:type="noteref">*7</a></sup>、さらなる性能向上も期待できます。</p>
<p>また、Mixtral や Grok-1 など、MoE アーキテクチャを採用している他のモデルと比較しても優れた点があります。前述したように、DBRX は、16 個のエキスパートの中から 4 つを選択します。これはつまり、1,820 通りのエキスパートの組み合わせを実現するという意味を持ちます。対して、Mixtral や Grok-1 は 8 個のエキスパートから 2 つを選択するという設計のため、組み合わせ総数は 28 通りです。よって、DBRX は 65 倍多くの組み合わせ総数を誇り、これが前述した高精度を実現する一因になっています。なお、このように、より多くの組み合わせを実現する当アプローチを「fine-grained MoE」（より細かな MoE ）と呼んでおり、これは開発過程における数多くの実験から得られた重要な成果の一つになります。</p>
<p>さて、そろそろ DBRX を使ってみたくなってきたのではないでしょうか？ DBRX は、データブリックス・ユーザーであれば AI Playground を通して簡単に利用できます<sup><a id="fnb-DBRX_AVAILABLE_REGION" href="#fn-DBRX_AVAILABLE_REGION" class="noteref" epub:type="noteref">*8</a></sup>し、それ以外の方々も HuggingFace Space や Perplexity.ai で試すことが可能です。なお、お試しの際にはぜひ日本語でプロンプトを入力してみてください。DBRX は日本語に特化して事前学習されているわけではないですが、データセットには一部日本語データも含まれていたため、日本語のやり取りが可能です。ただ、自由作文のような創造性を求めるタスクではなく、文章要約や RAG（Retrieval-Augmented Generation）のように、プロンプトの中に前提情報や参考情報を一緒に与えるような堅実なタスクに向いてます。したがって、お試しの際には Few-Shot プロンプティングなどを活用いただくと、より期待通りの結果が得られると思います。</p>
<p>では、そろそろ本コラムを締めましょう。生成AI技術は進化のスピードが速く、今後も続々と新しいモデルが登場するでしょう。新しいモデルをチェックする際は、精度や性能だけでなく、MoEの採用の有無など、アーキテクチャの観点からも注目してみてください。面白い発見があるかもしれません。また、OSS モデルの品質は日々向上し、選択肢も増えています。これにより、クローズドモデルからOSSへの移行が、これまで以上に現実的な選択肢となりえます。データブリックスにとっても、今回の DBRX の発表はゴールではなく、技術進歩のマイルストンに過ぎません。今後このモデルが業界の潮流の中でどのように進化していくか、非常に楽しみです。</p>
<p>Viva Open Source!</p>
<div class="footnote-list">
<div class="footnote" id="fn-JEITA" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*1] </span>JEITA, 生成 AI 市場の世界需要額見通しを発表, 2023年12月21日</p></div>
<div class="footnote" id="fn-A16Z" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*2] </span>Sarah Wang and Shangda Xu, 16 Changes to the Way Enterprises Are Building and Buying Generative AI, 2024/3/21</p></div>
<div class="footnote" id="fn-DBRX_SOTA_NOTE" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*3] </span>この記録は2024年4月18日に Meta Llama 3 が公開されるまで継続されました。</p></div>
<div class="footnote" id="fn-DBRX_BLOG" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*4] </span>Databricks, Introducing DBRX: A New State-of-the-Art Open LLM, 2024/3/27</p></div>
<div class="footnote" id="fn-MOE_VS_DENSE" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*5] </span>MoEと密なモデルのどちらが精度面で優れているかについての結論は出ていません。DBRXは、リリース当時、密なモデルに対して優位な精度を示していましたが、その後、新たな密なモデルである Meta Llama 3 によってより高い精度が達成されました。今後も両アーキテクチャによる熾烈な競争が繰り返されることが予想されるため、結論が出るのはもう少し先になると思われます。</p></div>
<div class="footnote" id="fn-DBRX_INFERENCE_OPT" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*6] </span>https://github.com/databricks/dbrx?tab=readme-ov-file#inference</p></div>
<div class="footnote" id="fn-DBRX_QUANTIZATION" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*7] </span>https://huggingface.co/mlx-community/dbrx-instruct-4bit</p></div>
<div class="footnote" id="fn-DBRX_AVAILABLE_REGION" epub:type="footnote"><p class="footnote"><span class="footnote-mark">[*8] </span>2024年5月8日現在では、AWSとAzureの一部のUSリージョンのみで提供中です。</p></div>
</div><!--/.footnote-list-->
</div>

        </main>
        <nav class="page-navi">
          <a href="dbsql-migration.html" class="page-prev">&#9664;</a>
          <a href="contributors.html" class="page-next">&#9654;</a>
        </nav>
        <footer>
        </footer>
      </div>
    </div>
  </body>
</html>
<!-- layout.html5.erb -->
